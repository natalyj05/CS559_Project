{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Final Project </center>\n",
    "## <center> Group 3 </center>\n",
    "\n",
    "## Elia Rezaeian\n",
    "\n",
    "_________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "### SECTION3 : Train Model-Stacking Method\n",
    "_________________________________________________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zy/_h1xy46d69jg35qmf_0x_vnw0000gn/T/ipykernel_26683/1064586460.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Section1&2(Clusters).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>Predicted_Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.658210</td>\n",
       "      <td>-1.400730</td>\n",
       "      <td>-0.381088</td>\n",
       "      <td>0.452337</td>\n",
       "      <td>0.858035</td>\n",
       "      <td>-0.012784</td>\n",
       "      <td>-0.381166</td>\n",
       "      <td>0.117564</td>\n",
       "      <td>0.221323</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.428676</td>\n",
       "      <td>0.461506</td>\n",
       "      <td>-1.030601</td>\n",
       "      <td>-0.234852</td>\n",
       "      <td>-0.029953</td>\n",
       "      <td>-0.033227</td>\n",
       "      <td>-0.566027</td>\n",
       "      <td>0.076135</td>\n",
       "      <td>0.440510</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.775424</td>\n",
       "      <td>1.442919</td>\n",
       "      <td>-0.711948</td>\n",
       "      <td>0.144689</td>\n",
       "      <td>-0.152528</td>\n",
       "      <td>0.164080</td>\n",
       "      <td>0.258936</td>\n",
       "      <td>-0.555779</td>\n",
       "      <td>-0.286176</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.403655</td>\n",
       "      <td>4.045684</td>\n",
       "      <td>0.326334</td>\n",
       "      <td>-1.356795</td>\n",
       "      <td>-0.638412</td>\n",
       "      <td>-0.146502</td>\n",
       "      <td>-0.058012</td>\n",
       "      <td>0.546713</td>\n",
       "      <td>-0.348422</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.283041</td>\n",
       "      <td>0.526507</td>\n",
       "      <td>-0.195726</td>\n",
       "      <td>-0.437119</td>\n",
       "      <td>-0.979682</td>\n",
       "      <td>-0.145639</td>\n",
       "      <td>-0.450059</td>\n",
       "      <td>0.824183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
       "0  2.658210 -1.400730 -0.381088  0.452337  0.858035 -0.012784 -0.381166   \n",
       "1 -1.428676  0.461506 -1.030601 -0.234852 -0.029953 -0.033227 -0.566027   \n",
       "2 -2.775424  1.442919 -0.711948  0.144689 -0.152528  0.164080  0.258936   \n",
       "3  2.403655  4.045684  0.326334 -1.356795 -0.638412 -0.146502 -0.058012   \n",
       "4  0.865429  0.283041  0.526507 -0.195726 -0.437119 -0.979682 -0.145639   \n",
       "\n",
       "        PC8       PC9  Cluster  Bankrupt?  Predicted_Cluster  \n",
       "0  0.117564  0.221323        1          0                  1  \n",
       "1  0.076135  0.440510        5          0                  5  \n",
       "2 -0.555779 -0.286176        5          0                  5  \n",
       "3  0.546713 -0.348422        0          0                  0  \n",
       "4 -0.450059  0.824183        0          0                  0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the data into X and Y and then I select companies in clusters 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-3]\n",
    "y = df.iloc[:,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = X[df.iloc[:,-1] == 0] \n",
    "X_1 = X[df.iloc[:,-1] == 1]\n",
    "\n",
    "y_0 = y[df.iloc[:,-1] == 0]\n",
    "y_1 = y[df.iloc[:,-1] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I wrote a function for models Tuning that we can apply on differtent data and scoring using Stratified Kfold and Gridsearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that have all base models with tuning their hyperparameters using grid search\n",
    "\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['Logistic'] = LogisticRegression(solver='liblinear',random_state=42)\n",
    "    models['SVM'] = SVC(random_state=42)\n",
    "    models['KNN'] = KNeighborsClassifier()\n",
    "    models['RandomForest'] = RandomForestClassifier(random_state=42)\n",
    "    models['xgboost'] = xgb.XGBClassifier(random_state=42)\n",
    "    models['NeuralNetwork'] = MLPClassifier(random_state=42)\n",
    "    models['DecisionTree'] = DecisionTreeClassifier(random_state=42)\n",
    "    models['AdaBoost'] = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=42),random_state=42)\n",
    "    models['GradientBoosting'] = GradientBoostingClassifier(random_state=42)\n",
    "    models['NaiveBayes'] = GaussianNB()\n",
    "    models['Bagging'] = BaggingClassifier(random_state=42)\n",
    "    \n",
    "    return models\n",
    "\n",
    "def get_params():\n",
    "    \n",
    "    params = dict()\n",
    "    params['Logistic'] =  { 'penalty': ['l1', 'l2', 'elasticnet', 'none'],'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'class_weight': ['balanced', None], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],'max_iter': [100, 200, 300, 400, 500]}\n",
    "    \n",
    "    params['SVM'] = {'C': [ 0.01, 0.1, 1 ], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],'degree': [2, 3, 4, 5],\n",
    "    'gamma': ['scale', 'auto'], 'class_weight': ['balanced']}\n",
    "    \n",
    "    params['KNN'] = {'n_neighbors': [3, 5, 7, 9, 11], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50],'p': [1, 2]}\n",
    "    \n",
    "    params['RandomForest'] = param_grid = {'n_estimators': [100, 200, 300, 400], 'max_depth': [None, 10, 20, 30, 40],   \n",
    "    'min_samples_split': [2, 5, 10],'min_samples_leaf': [1, 2, 4],'bootstrap': [True, False],'class_weight': ['balanced', 'balanced_subsample'],  \n",
    "    'max_features': ['auto', 'sqrt', 'log2']}\n",
    "    \n",
    "    params['xgboost'] = {'max_depth': [3, 4, 5, 6],'min_child_weight': [1, 2, 3],'gamma': [0, 0.1, 0.2, 0.3],'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],'learning_rate': [0.01, 0.1, 0.2, 0.3],'n_estimators': [100, 200, 300]}\n",
    "    \n",
    "    params['NeuralNetwork'] = {'hidden_layer_sizes': [(100,), (50, 50), (100, 100), (50, 50, 50)],\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],'solver': ['lbfgs', 'sgd', 'adam'],'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],'max_iter': [200, 300, 400]}\n",
    "    \n",
    "    params['DecisionTree'] = {'max_depth': [None, 10, 20, 30, 50], 'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],'criterion': ['gini', 'entropy']}\n",
    "    \n",
    "    params['AdaBoost'] = {'n_estimators': [50, 100, 200, 300],'learning_rate': [0.01, 0.1, 1, 10],\n",
    "    'base_estimator__max_depth': [1, 2, 3]}\n",
    "    \n",
    "    params['GradientBoosting'] = {'n_estimators': [100, 200, 300, 400],'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "    'max_depth': [3, 5, 8],'min_samples_split': [2, 5, 10],'min_samples_leaf': [1, 2, 4],'max_features': ['sqrt', 'log2', None]}\n",
    "    \n",
    "    params['NaiveBayes'] = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
    "    \n",
    "    params['Bagging'] = {'n_estimators': [10, 50, 100, 200],'max_samples': [0.5, 0.7, 1.0], 'max_features': [0.5, 0.7, 1.0],  \n",
    "    'bootstrap': [True, False],'bootstrap_features': [True, False]}\n",
    "    \n",
    "    return params\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "def get_best_model(X, y, models, params, score):\n",
    "    best_model = dict()\n",
    "    for key in models.keys():\n",
    "        model = models[key]\n",
    "        param = params[key]\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle = True, random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param, n_jobs=-1, cv=cv, scoring = score)\n",
    "        grid_result = grid_search.fit(X, y)\n",
    "        best_model[key] = grid_result.best_estimator_\n",
    "        # Print the best model and best score for each type of model\n",
    "        print(f\"Best Model for {key}: {grid_result.best_estimator_}\")\n",
    "        print(f\"Best Parameters {key}: {grid_result.best_params_}\")\n",
    "        print(f\"Best Score for {key}: {grid_result.best_score_:.4f}\\n\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter tuning for cluster 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:925: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.         ...        nan 0.03703704 0.03703704]\n",
      "  category=UserWarning\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model for Logistic: LogisticRegression(C=0.1, class_weight='balanced', random_state=42,\n",
      "                   solver='saga')\n",
      "Best Parameters Logistic: {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 100, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Best Score for Logistic: 0.2413\n",
      "\n",
      "Best Model for SVM: SVC(C=0.01, class_weight='balanced', degree=5, kernel='poly', random_state=42)\n",
      "Best Parameters SVM: {'C': 0.01, 'class_weight': 'balanced', 'degree': 5, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Best Score for SVM: 0.2405\n",
      "\n",
      "Best Model for KNN: KNeighborsClassifier(leaf_size=10, n_neighbors=9, p=1)\n",
      "Best Parameters KNN: {'algorithm': 'auto', 'leaf_size': 10, 'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}\n",
      "Best Score for KNN: 0.0784\n",
      "\n",
      "Best Model for RandomForest: RandomForestClassifier(class_weight='balanced_subsample', max_depth=10,\n",
      "                       min_samples_leaf=4, min_samples_split=10,\n",
      "                       random_state=42)\n",
      "Best Parameters RandomForest: {'bootstrap': True, 'class_weight': 'balanced_subsample', 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Best Score for RandomForest: 0.1652\n",
      "\n",
      "Best Model for xgboost: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=2,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n",
      "Best Parameters xgboost: {'colsample_bytree': 0.8, 'gamma': 0.2, 'learning_rate': 0.3, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best Score for xgboost: 0.1937\n",
      "\n",
      "Best Model for NeuralNetwork: MLPClassifier(activation='tanh', alpha=0.001, hidden_layer_sizes=(100, 100),\n",
      "              random_state=42, solver='lbfgs')\n",
      "Best Parameters NeuralNetwork: {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'max_iter': 200, 'solver': 'lbfgs'}\n",
      "Best Score for NeuralNetwork: 0.2806\n",
      "\n",
      "Best Model for DecisionTree: DecisionTreeClassifier(criterion='entropy', min_samples_leaf=2, random_state=42)\n",
      "Best Parameters DecisionTree: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best Score for DecisionTree: 0.1777\n",
      "\n",
      "Best Model for AdaBoost: AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3,\n",
      "                                                         random_state=42),\n",
      "                   learning_rate=1, n_estimators=200, random_state=42)\n",
      "Best Parameters AdaBoost: {'base_estimator__max_depth': 3, 'learning_rate': 1, 'n_estimators': 200}\n",
      "Best Score for AdaBoost: 0.1303\n",
      "\n",
      "Best Model for GradientBoosting: GradientBoostingClassifier(learning_rate=0.5, max_features='sqrt',\n",
      "                           min_samples_leaf=2, min_samples_split=5,\n",
      "                           n_estimators=400, random_state=42)\n",
      "Best Parameters GradientBoosting: {'learning_rate': 0.5, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 400}\n",
      "Best Score for GradientBoosting: 0.1908\n",
      "\n",
      "Best Model for NaiveBayes: GaussianNB()\n",
      "Best Parameters NaiveBayes: {'var_smoothing': 1e-09}\n",
      "Best Score for NaiveBayes: 0.2455\n",
      "\n",
      "Best Model for Bagging: BaggingClassifier(bootstrap=False, bootstrap_features=True, random_state=42)\n",
      "Best Parameters Bagging: {'bootstrap': False, 'bootstrap_features': True, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 10}\n",
      "Best Score for Bagging: 0.1005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#apply on cluster 0\n",
    "\n",
    "models_0 = get_models()\n",
    "params_0 = get_params()\n",
    "best_model_0 = get_best_model(X_0, y_0, models_0, params_0, 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I selected 3 best model from the previous cell and apply a meta learner using logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression confustion matrix:\n",
      " [[908 275]\n",
      " [ 12  36]]\n",
      "SVM confustion matrix:\n",
      " [[1047  136]\n",
      " [  18   30]]\n",
      "NN confustion matrix:\n",
      " [[1183    0]\n",
      " [   0   48]]\n",
      "Meta learner Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1183\n",
      "           1       1.00      1.00      1.00        48\n",
      "\n",
      "    accuracy                           1.00      1231\n",
      "   macro avg       1.00      1.00      1.00      1231\n",
      "weighted avg       1.00      1.00      1.00      1231\n",
      "\n",
      "Meta learner Confusion Matrix:\n",
      " [[1183    0]\n",
      " [   0   48]]\n",
      "Meta learner Accuracy:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "# Cluster 0 \n",
    "# Best models for cluster 0 which we can use in our meta model\n",
    "LR_0 = best_model_0['Logistic']\n",
    "SVM_0 = best_model_0['SVM']\n",
    "NN_0 = best_model_0['NeuralNetwork']\n",
    "\n",
    "model_0_1 = LR_0.fit(X_0,y_0)\n",
    "model_0_2 = SVM_0.fit(X_0,y_0)\n",
    "model_0_3 = NN_0.fit(X_0,y_0)\n",
    "\n",
    "LR_pred_0 = model_0_1.predict(X_0)\n",
    "SVM_pred_0 = model_0_2.predict(X_0)\n",
    "NN_pred_0 = model_0_3.predict(X_0)\n",
    "\n",
    "print(\"Logistic Regression confustion matrix:\\n\",confusion_matrix(y_0,LR_pred_0 ))\n",
    "print(\"SVM confustion matrix:\\n\",confusion_matrix(y_0,SVM_pred_0 ))\n",
    "print(\"NN confustion matrix:\\n\",confusion_matrix(y_0,NN_pred_0 ))\n",
    "\n",
    "X_Stack_0 = pd.DataFrame()\n",
    "X_Stack_0['NN'] = NN_pred_0\n",
    "X_Stack_0['SVM'] = SVM_pred_0\n",
    "X_Stack_0['LR'] = LR_pred_0\n",
    "\n",
    "meta_learner = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "meta_learner.fit(X_Stack_0, y_0)\n",
    "\n",
    "meta_learner_pred = meta_learner.predict(X_Stack_0)\n",
    "meta_learner_classification_report = classification_report(y_0, meta_learner_pred)\n",
    "print(\"Meta learner Classification report:\\n\",meta_learner_classification_report)\n",
    "meta_learner_cm = confusion_matrix(y_0, meta_learner_pred)\n",
    "print(\"Meta learner Confusion Matrix:\\n\",meta_learner_cm)\n",
    "accuracy_score_meta1 = accuracy_score(y_0, meta_learner_pred)\n",
    "print(\"Meta learner Accuracy:\\n\", accuracy_score_meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The TT/(TT+TF) for logistic regression is = > 75%\n",
    "* The TT/(TT+TF) for SVM is = > 62%\n",
    "* The TT/(TT+TF) Neural Network is = > 100% \n",
    "\n",
    "when I use meta learner in the for ensemble stacking, I got 100% accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here I tune the hyperparameters on Cluster 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:925: UserWarning: One or more of the test scores are non-finite: [nan nan  0. ... nan  0.  0.]\n",
      "  category=UserWarning\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model for Logistic: LogisticRegression(C=0.01, class_weight='balanced', max_iter=300, penalty='l1',\n",
      "                   random_state=42, solver='saga')\n",
      "Best Parameters Logistic: {'C': 0.01, 'class_weight': 'balanced', 'max_iter': 300, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best Score for Logistic: 0.1752\n",
      "\n",
      "Best Model for SVM: SVC(C=0.1, class_weight='balanced', degree=5, kernel='poly', random_state=42)\n",
      "Best Parameters SVM: {'C': 0.1, 'class_weight': 'balanced', 'degree': 5, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Best Score for SVM: 0.2000\n",
      "\n",
      "Best Model for KNN: KNeighborsClassifier(leaf_size=10, n_neighbors=3, p=1)\n",
      "Best Parameters KNN: {'algorithm': 'auto', 'leaf_size': 10, 'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}\n",
      "Best Score for KNN: 0.0000\n",
      "\n",
      "Best Model for RandomForest: RandomForestClassifier(class_weight='balanced', random_state=42)\n",
      "Best Parameters RandomForest: {'bootstrap': True, 'class_weight': 'balanced', 'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Score for RandomForest: 0.0000\n",
      "\n",
      "Best Model for xgboost: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.01, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n",
      "Best Parameters xgboost: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best Score for xgboost: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model for NeuralNetwork: MLPClassifier(activation='tanh', max_iter=300, random_state=42)\n",
      "Best Parameters NeuralNetwork: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 300, 'solver': 'adam'}\n",
      "Best Score for NeuralNetwork: 0.1667\n",
      "\n",
      "Best Model for DecisionTree: DecisionTreeClassifier(random_state=42)\n",
      "Best Parameters DecisionTree: {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Score for DecisionTree: 0.0000\n",
      "\n",
      "Best Model for AdaBoost: AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3,\n",
      "                                                         random_state=42),\n",
      "                   learning_rate=10, random_state=42)\n",
      "Best Parameters AdaBoost: {'base_estimator__max_depth': 3, 'learning_rate': 10, 'n_estimators': 50}\n",
      "Best Score for AdaBoost: 0.0558\n",
      "\n",
      "Best Model for GradientBoosting: GradientBoostingClassifier(learning_rate=0.5, max_features='sqrt',\n",
      "                           n_estimators=200, random_state=42)\n",
      "Best Parameters GradientBoosting: {'learning_rate': 0.5, 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Score for GradientBoosting: 0.1333\n",
      "\n",
      "Best Model for NaiveBayes: GaussianNB()\n",
      "Best Parameters NaiveBayes: {'var_smoothing': 1e-09}\n",
      "Best Score for NaiveBayes: 0.0000\n",
      "\n",
      "Best Model for Bagging: BaggingClassifier(bootstrap_features=True, max_features=0.5, max_samples=0.5,\n",
      "                  random_state=42)\n",
      "Best Parameters Bagging: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 10}\n",
      "Best Score for Bagging: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#apply on cluster 1\n",
    "\n",
    "models_1 = get_models()\n",
    "params_1 = get_params()\n",
    "best_model_1 = get_best_model(X_1, y_1, models_1, params_1, 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I selected 3 best model from the previous cell and apply a meta learner using logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression confustion matrix:\n",
      " [[317 218]\n",
      " [  2   8]]\n",
      "SVM confustion matrix:\n",
      " [[514  21]\n",
      " [  4   6]]\n",
      "NN confustion matrix:\n",
      " [[535   0]\n",
      " [  0  10]]\n",
      "Meta learner Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       535\n",
      "           1       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00       545\n",
      "   macro avg       1.00      1.00      1.00       545\n",
      "weighted avg       1.00      1.00      1.00       545\n",
      "\n",
      "Meta learner Confusion Matrix:\n",
      " [[535   0]\n",
      " [  0  10]]\n",
      "Meta learner Accuracy:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "# Best models for cluster 0 which we can use in our meta model\n",
    "LR_1 = best_model_0['Logistic']\n",
    "SVM_1 = best_model_0['SVM']\n",
    "NN_0 = best_model_0['NeuralNetwork']\n",
    "\n",
    "model_1_1 = LR_0.fit(X_1,y_1)\n",
    "model_1_2 = SVM_0.fit(X_1,y_1)\n",
    "model_1_3 = NN_0.fit(X_1,y_1)\n",
    "\n",
    "LR_pred_1 = model_1_1.predict(X_1)\n",
    "SVM_pred_1 = model_1_2.predict(X_1)\n",
    "NN_pred_1 = model_1_3.predict(X_1)\n",
    "\n",
    "print(\"Logistic Regression confustion matrix:\\n\",confusion_matrix(y_1,LR_pred_1 ))\n",
    "print(\"SVM confustion matrix:\\n\",confusion_matrix(y_1,SVM_pred_1 ))\n",
    "print(\"NN confustion matrix:\\n\",confusion_matrix(y_1,NN_pred_1 ))\n",
    "\n",
    "X_Stack_1 = pd.DataFrame()\n",
    "X_Stack_1['NN'] = NN_pred_1\n",
    "X_Stack_1['SVM'] = SVM_pred_1\n",
    "X_Stack_1['LR'] = LR_pred_1\n",
    "\n",
    "meta_learner1 = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "meta_learner1.fit(X_Stack_1, y_1)\n",
    "\n",
    "meta_learner_pred1 = meta_learner1.predict(X_Stack_1)\n",
    "meta_learner_classification_report1 = classification_report(y_1, meta_learner_pred1)\n",
    "print(\"Meta learner Classification report:\\n\",meta_learner_classification_report1)\n",
    "meta_learner_cm1 = confusion_matrix(y_1, meta_learner_pred1)\n",
    "print(\"Meta learner Confusion Matrix:\\n\",meta_learner_cm1)\n",
    "accuracy_score_meta1 = accuracy_score(y_1, meta_learner_pred1)\n",
    "print(\"Meta learner Accuracy:\\n\", accuracy_score_meta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The TT/(TT+TF) for logistic regression is = > 80%\n",
    "* The TT/(TT+TF) for SVM is = > 60%\n",
    "* The TT/(TT+TF) Neural Network is = > 100% \n",
    "\n",
    "when I use meta learner in the for ensemble stacking, I got 100% accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________________________________________________\n",
    "### SECTION4 : K_Fold Cross Validation on the Whole Dataset\n",
    "_________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>Predicted_Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.658210</td>\n",
       "      <td>-1.400730</td>\n",
       "      <td>-0.381088</td>\n",
       "      <td>0.452337</td>\n",
       "      <td>0.858035</td>\n",
       "      <td>-0.012784</td>\n",
       "      <td>-0.381166</td>\n",
       "      <td>0.117564</td>\n",
       "      <td>0.221323</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.428676</td>\n",
       "      <td>0.461506</td>\n",
       "      <td>-1.030601</td>\n",
       "      <td>-0.234852</td>\n",
       "      <td>-0.029953</td>\n",
       "      <td>-0.033227</td>\n",
       "      <td>-0.566027</td>\n",
       "      <td>0.076135</td>\n",
       "      <td>0.440510</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.775424</td>\n",
       "      <td>1.442919</td>\n",
       "      <td>-0.711948</td>\n",
       "      <td>0.144689</td>\n",
       "      <td>-0.152528</td>\n",
       "      <td>0.164080</td>\n",
       "      <td>0.258936</td>\n",
       "      <td>-0.555779</td>\n",
       "      <td>-0.286176</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.403655</td>\n",
       "      <td>4.045684</td>\n",
       "      <td>0.326334</td>\n",
       "      <td>-1.356795</td>\n",
       "      <td>-0.638412</td>\n",
       "      <td>-0.146502</td>\n",
       "      <td>-0.058012</td>\n",
       "      <td>0.546713</td>\n",
       "      <td>-0.348422</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.283041</td>\n",
       "      <td>0.526507</td>\n",
       "      <td>-0.195726</td>\n",
       "      <td>-0.437119</td>\n",
       "      <td>-0.979682</td>\n",
       "      <td>-0.145639</td>\n",
       "      <td>-0.450059</td>\n",
       "      <td>0.824183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
       "0  2.658210 -1.400730 -0.381088  0.452337  0.858035 -0.012784 -0.381166   \n",
       "1 -1.428676  0.461506 -1.030601 -0.234852 -0.029953 -0.033227 -0.566027   \n",
       "2 -2.775424  1.442919 -0.711948  0.144689 -0.152528  0.164080  0.258936   \n",
       "3  2.403655  4.045684  0.326334 -1.356795 -0.638412 -0.146502 -0.058012   \n",
       "4  0.865429  0.283041  0.526507 -0.195726 -0.437119 -0.979682 -0.145639   \n",
       "\n",
       "        PC8       PC9  Cluster  Bankrupt?  Predicted_Cluster  \n",
       "0  0.117564  0.221323        1          0                  1  \n",
       "1  0.076135  0.440510        5          0                  5  \n",
       "2 -0.555779 -0.286176        5          0                  5  \n",
       "3  0.546713 -0.348422        0          0                  0  \n",
       "4 -0.450059  0.824183        0          0                  0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('Section1&2(Clusters).csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:9]\n",
    "y = df.iloc[:,10]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here I first run all models without tuning on the whole dataset and got results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result_whole_data1 = {}\n",
    "RANDOM_STATE = 42\n",
    "# Initialize the classifier\n",
    "models1 = {\n",
    "    'LogisticRegression': LogisticRegression(solver='liblinear', random_state=42),\n",
    "    'SVC': SVC(random_state=42),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "    'XGBClassifier': xgb.XGBClassifier(random_state=42),\n",
    "    'MLPClassifier': MLPClassifier(random_state=42),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(random_state=42),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'BaggingClassifier': BaggingClassifier(random_state=42)\n",
    "}\n",
    "# Use stratified kfold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True ,random_state=RANDOM_STATE)\n",
    "\n",
    "for model_name, model in models1.items():\n",
    "    recall_scores = cross_val_score(model, X, y, cv=skf, scoring='recall')\n",
    "    mean_scores = recall_scores.mean()\n",
    "    # find cm\n",
    "    y_pred = cross_val_predict(model, X, y, cv=skf)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    result_whole_data1[model_name] = (cm, mean_scores, accuracy)\n",
    "\n",
    "# print(\"Results for whole data:\")\n",
    "# print(result_whole_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here shows that while the accuracy is high but they cannot predict the Instanses with label 1 well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression confustion matrix:\n",
      " [[5582   27]\n",
      " [ 162   36]]\n",
      "TT/(TT+TF) =  0.18166666666666667\n",
      "Accuracy =  0.9674530738763562\n",
      "-------------------------------------\n",
      "SVC confustion matrix:\n",
      " [[5604    5]\n",
      " [ 188   10]]\n",
      "TT/(TT+TF) =  0.05051282051282051\n",
      "Accuracy =  0.9667642500430514\n",
      "-------------------------------------\n",
      "KNeighborsClassifier confustion matrix:\n",
      " [[5558   51]\n",
      " [ 151   47]]\n",
      "TT/(TT+TF) =  0.23756410256410257\n",
      "Accuracy =  0.965214396418116\n",
      "-------------------------------------\n",
      "RandomForestClassifier confustion matrix:\n",
      " [[5583   26]\n",
      " [ 159   39]]\n",
      "TT/(TT+TF) =  0.1971794871794872\n",
      "Accuracy =  0.9681418977096607\n",
      "-------------------------------------\n",
      "XGBClassifier confustion matrix:\n",
      " [[5562   47]\n",
      " [ 150   48]]\n",
      "TT/(TT+TF) =  0.2421794871794872\n",
      "Accuracy =  0.9660754262097468\n",
      "-------------------------------------\n",
      "MLPClassifier confustion matrix:\n",
      " [[5575   34]\n",
      " [ 152   46]]\n",
      "TT/(TT+TF) =  0.23230769230769227\n",
      "Accuracy =  0.9679696917513346\n",
      "-------------------------------------\n",
      "DecisionTreeClassifier confustion matrix:\n",
      " [[5463  146]\n",
      " [ 140   58]]\n",
      "TT/(TT+TF) =  0.2928205128205128\n",
      "Accuracy =  0.9507490959187188\n",
      "-------------------------------------\n",
      "AdaBoostClassifier confustion matrix:\n",
      " [[5558   51]\n",
      " [ 147   51]]\n",
      "TT/(TT+TF) =  0.25769230769230766\n",
      "Accuracy =  0.9659032202514207\n",
      "-------------------------------------\n",
      "GradientBoostingClassifier confustion matrix:\n",
      " [[5560   49]\n",
      " [ 154   44]]\n",
      "TT/(TT+TF) =  0.22243589743589745\n",
      "Accuracy =  0.9650421904597899\n",
      "-------------------------------------\n",
      "GaussianNB confustion matrix:\n",
      " [[5480  129]\n",
      " [ 114   84]]\n",
      "TT/(TT+TF) =  0.4244871794871795\n",
      "Accuracy =  0.9581539521267436\n",
      "-------------------------------------\n",
      "BaggingClassifier confustion matrix:\n",
      " [[5561   48]\n",
      " [ 158   40]]\n",
      "TT/(TT+TF) =  0.20102564102564102\n",
      "Accuracy =  0.9645255725848114\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"LogisticRegression confustion matrix:\\n\", result_whole_data1['LogisticRegression'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['LogisticRegression'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['LogisticRegression'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"SVC confustion matrix:\\n\", result_whole_data1['SVC'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['SVC'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['SVC'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"KNeighborsClassifier confustion matrix:\\n\", result_whole_data1['KNeighborsClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['KNeighborsClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['KNeighborsClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"RandomForestClassifier confustion matrix:\\n\", result_whole_data1['RandomForestClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['RandomForestClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['RandomForestClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"XGBClassifier confustion matrix:\\n\", result_whole_data1['XGBClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['XGBClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['XGBClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"MLPClassifier confustion matrix:\\n\", result_whole_data1['MLPClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['MLPClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['MLPClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"DecisionTreeClassifier confustion matrix:\\n\", result_whole_data1['DecisionTreeClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['DecisionTreeClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['DecisionTreeClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"AdaBoostClassifier confustion matrix:\\n\", result_whole_data1['AdaBoostClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['AdaBoostClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['AdaBoostClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"GradientBoostingClassifier confustion matrix:\\n\", result_whole_data1['GradientBoostingClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['GradientBoostingClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['GradientBoostingClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"GaussianNB confustion matrix:\\n\", result_whole_data1['GaussianNB'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['GaussianNB'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['GaussianNB'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"BaggingClassifier confustion matrix:\\n\", result_whole_data1['BaggingClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['BaggingClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['BaggingClassifier'][2])\n",
    "print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we tried to tune the hyperparameter of all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that have all base models with tuning their hyperparameters using grid search\n",
    "\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['Logistic'] = LogisticRegression(solver='liblinear',random_state=42)\n",
    "    models['SVM'] = SVC(random_state=42)\n",
    "    models['KNN'] = KNeighborsClassifier()\n",
    "    models['RandomForest'] = RandomForestClassifier(random_state=42)\n",
    "    models['xgboost'] = xgb.XGBClassifier(random_state=42)\n",
    "    models['NeuralNetwork'] = MLPClassifier(random_state=42)\n",
    "    models['AdaBoost'] = AdaBoostClassifier(random_state=42)\n",
    "    models['gradiantboost'] = GradientBoostingClassifier(random_state=42)\n",
    "    models['Bagging'] = BaggingClassifier(random_state=42)\n",
    "\n",
    "    \n",
    "    return models\n",
    "\n",
    "def get_params():\n",
    "    \n",
    "    params = dict()\n",
    "    params['Logistic'] =  { 'penalty': ['l1', 'l2', 'elasticnet', 'none'],'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'class_weight': ['balanced', None], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],'max_iter': [100, 200, 300, 400, 500]}\n",
    "    \n",
    "    params['SVM'] = {'C': [ 0.01, 0.1, 1 ], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],'degree': [2, 3, 4, 5],\n",
    "    'gamma': ['scale', 'auto'], 'class_weight': ['balanced']}\n",
    "    \n",
    "    params['KNN'] = {'n_neighbors': [3, 5, 7, 9, 11], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50],'p': [1, 2]}\n",
    "    \n",
    "    params['RandomForest'] = param_grid = {'n_estimators': [100, 200, 300, 400], 'max_depth': [None, 10, 20, 30, 40],   \n",
    "    'min_samples_split': [2, 5, 10],'min_samples_leaf': [1, 2, 4],'bootstrap': [True],'class_weight': ['balanced'],  \n",
    "    'max_features': ['auto', 'sqrt', 'log2']}\n",
    "    \n",
    "    params['NeuralNetwork'] = {'hidden_layer_sizes': [(100,), (50, 50), (100, 100), (50, 50, 50)],\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],'solver': ['lbfgs', 'sgd', 'adam'],'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],'max_iter': [200, 300, 400]}\n",
    "    \n",
    "    params['xgboost'] = {'max_depth': [3, 4, 5, 6],'min_child_weight': [1, 2, 3],'gamma': [0, 0.1, 0.2, 0.3],'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],'learning_rate': [0.01, 0.1, 0.2, 0.3],'n_estimators': [100, 200, 300]}\n",
    "    \n",
    "    params['AdaBoost'] = {'n_estimators': [50, 100, 200, 300],'learning_rate': [0.01, 0.1, 1, 10],'algorithm': ['SAMME', 'SAMME.R']} \n",
    "  \n",
    "    params['gradiantboost'] = {'n_estimators': [50, 100, 200, 300],'learning_rate': [0.01, 0.1, 1, 10],'max_depth': [3, 4, 5, 6]}\n",
    "    \n",
    "    params['Bagging'] = {'n_estimators': [10, 50, 100, 200],'max_samples': [0.5, 1.0],'max_features': [0.5, 1.0]}   \n",
    "        \n",
    "    return params\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "def get_best_model(X, y, models, params, score):\n",
    "    best_model = dict()\n",
    "    for key in models.keys():\n",
    "        model = models[key]\n",
    "        param = params[key]\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle= True, random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param, n_jobs=-1, cv=cv, scoring = score)\n",
    "        grid_result = grid_search.fit(X, y)\n",
    "        best_model[key] = grid_result.best_estimator_\n",
    "        # Print the best model and best score for each type of model\n",
    "        print(f\"Best Model for {key}: {grid_result.best_estimator_}\")\n",
    "        print(f\"Best Parameters {key}: {grid_result.best_params_}\")\n",
    "        print(f\"Best Score for {key}: {grid_result.best_score_:.4f}\\n\")\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model for AdaBoost: AdaBoostClassifier(learning_rate=1, n_estimators=200, random_state=42)\n",
      "Best Parameters AdaBoost: {'algorithm': 'SAMME.R', 'learning_rate': 1, 'n_estimators': 200}\n",
      "Best Score for AdaBoost: 0.3627\n",
      "\n",
      "Best Model for gradiantboost: GradientBoostingClassifier(learning_rate=1, n_estimators=50, random_state=42)\n",
      "Best Parameters gradiantboost: {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best Score for gradiantboost: 0.3485\n",
      "\n",
      "Best Model for Bagging: BaggingClassifier(max_samples=0.5, n_estimators=200, random_state=42)\n",
      "Best Parameters Bagging: {'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 200}\n",
      "Best Score for Bagging: 0.2845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = get_models()\n",
    "params = get_params()\n",
    "best_model= get_best_model(X, y, models, params, 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so I used the tuned models for the k fold cross validation and got much better results here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/elia/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use kfold on whole data with random forest, XGboost and Bagging\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "result_whole_data = {}\n",
    "RANDOM_STATE = 42\n",
    "# Initialize the classifier\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced', min_samples_leaf=4,min_samples_split=10, n_estimators=200, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=RANDOM_STATE, colsample_bytree= 0.9, gamma =0.2, learning_rate= 0.3, max_depth = 4, min_child_weight= 3, n_estimators= 300, subsample= 0.8),\n",
    "    'Bagging': BaggingClassifier(random_state=RANDOM_STATE),\n",
    "    'Logistic Regression': LogisticRegression(C=1, penalty='l1', random_state=42, solver='liblinear'),\n",
    "    \"SVM\" : SVC(C=0.01, class_weight='balanced', degree=5, kernel='poly', random_state=42),\n",
    "    'KNN': KNeighborsClassifier(leaf_size=10, p=1, weights='distance'),\n",
    "    'NN':MLPClassifier(activation = 'relu', alpha= 0.0001, hidden_layer_sizes= (100,), learning_rate= 'constant', max_iter= 400, solver= 'adam', random_state = 42),\n",
    "    'AdaBoost' : AdaBoostClassifier(algorithm='SAMME.R', learning_rate=1, n_estimators=50, random_state=42),\n",
    "    'GradientBoost': GradientBoostingClassifier(learning_rate=1, max_depth=3, n_estimators=50, random_state=42),\n",
    "    'Bagging': BaggingClassifier(max_features=1, max_samples=0.5, n_estimators=200, random_state=42)\n",
    "}\n",
    "# Use stratified kfold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    recall_scores = cross_val_score(model, X, y, cv=skf, scoring='recall')\n",
    "    mean_scores = recall_scores.mean()\n",
    "    # find cm\n",
    "    y_pred = cross_val_predict(model, X, y, cv=skf)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    result_whole_data[model_name] = (cm, mean_scores, acc)\n",
    "\n",
    "# print(\"Results for whole data:\")\n",
    "# print(result_whole_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest confustion matrix:\n",
      " [[5494  115]\n",
      " [ 116   82]]\n",
      "TT/(TT+TF) =  0.4142307692307693\n",
      "accuracy: 0.9602204236266575\n",
      "------------------------------------\n",
      "XGBoost confustion matrix:\n",
      " [[5554   55]\n",
      " [ 153   45]]\n",
      "TT/(TT+TF) =  0.22717948717948716\n",
      "accuracy: 0.9641811606681591\n",
      "------------------------------------\n",
      "Bagging confustion matrix:\n",
      " [[5609    0]\n",
      " [ 198    0]]\n",
      "TT/(TT+TF) =  0.0\n",
      "accuracy: 0.9659032202514207\n",
      "------------------------------------\n",
      "Logistic Regression confustion matrix:\n",
      " [[5582   27]\n",
      " [ 162   36]]\n",
      "TT/(TT+TF) =  0.18166666666666667\n",
      "accuracy: 0.9674530738763562\n",
      "------------------------------------\n",
      "SVM confustion matrix:\n",
      " [[5191  418]\n",
      " [  49  149]]\n",
      "TT/(TT+TF) =  0.7521794871794871\n",
      "accuracy: 0.9195798174616842\n",
      "------------------------------------\n",
      "KNN confustion matrix:\n",
      " [[5560   49]\n",
      " [ 156   42]]\n",
      "TT/(TT+TF) =  0.21217948717948723\n",
      "accuracy: 0.9646977785431375\n",
      "------------------------------------\n",
      "NN confustion matrix:\n",
      " [[5553   56]\n",
      " [ 141   57]]\n",
      "TT/(TT+TF) =  0.2878205128205128\n",
      "accuracy: 0.9660754262097468\n",
      "------------------------------------\n",
      "AdaBoost confustion matrix:\n",
      " [[5558   51]\n",
      " [ 147   51]]\n",
      "TT/(TT+TF) =  0.25769230769230766\n",
      "accuracy: 0.9659032202514207\n",
      "------------------------------------\n",
      "GradientBoost confustion matrix:\n",
      " [[5404  205]\n",
      " [ 120   78]]\n",
      "TT/(TT+TF) =  0.39282051282051283\n",
      "accuracy: 0.9440330635439986\n",
      "------------------------------------\n",
      "Bagging confustion matrix:\n",
      " [[5609    0]\n",
      " [ 198    0]]\n",
      "TT/(TT+TF) =  0.0\n",
      "accuracy: 0.9659032202514207\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest confustion matrix:\\n\", result_whole_data['Random Forest'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['Random Forest'][1])\n",
    "print('accuracy:', result_whole_data['Random Forest'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"XGBoost confustion matrix:\\n\", result_whole_data['XGBoost'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['XGBoost'][1])\n",
    "print('accuracy:', result_whole_data['XGBoost'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"Bagging confustion matrix:\\n\", result_whole_data['Bagging'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['Bagging'][1])\n",
    "print('accuracy:', result_whole_data['Bagging'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"Logistic Regression confustion matrix:\\n\", result_whole_data['Logistic Regression'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['Logistic Regression'][1])\n",
    "print('accuracy:', result_whole_data['Logistic Regression'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"SVM confustion matrix:\\n\", result_whole_data['SVM'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['SVM'][1])\n",
    "print('accuracy:', result_whole_data['SVM'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"KNN confustion matrix:\\n\", result_whole_data['KNN'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['KNN'][1])\n",
    "print('accuracy:', result_whole_data['KNN'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"NN confustion matrix:\\n\", result_whole_data['NN'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['NN'][1])\n",
    "print('accuracy:', result_whole_data['NN'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"AdaBoost confustion matrix:\\n\", result_whole_data['AdaBoost'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['AdaBoost'][1])\n",
    "print('accuracy:', result_whole_data['AdaBoost'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"GradientBoost confustion matrix:\\n\", result_whole_data['GradientBoost'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['GradientBoost'][1])\n",
    "print('accuracy:', result_whole_data['GradientBoost'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"Bagging confustion matrix:\\n\", result_whole_data['Bagging'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['Bagging'][1])\n",
    "print('accuracy:', result_whole_data['Bagging'][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as you can see, SVM has the best results. \n",
    "\n",
    "*SVM confustion matrix:\\\n",
    "\n",
    "[[5191      418]\\\n",
    "[  49       149]]\n",
    "\n",
    "* TT/(TT+TF) =  0.7521794871794871\n",
    "* accuracy: 0.9195798174616842"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
